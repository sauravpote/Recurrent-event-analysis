{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff953ee-23b9-45c2-a27e-0c3239b266ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATEST AND LAST UPDATES HAVING THE THIRD TAB\n",
    "###########UPDATE ON 10TH NOVEMBER ################\n",
    "# to incorporate the terminol0gies also \n",
    "####latest edited codes with a new front end #####################\n",
    "import tkinter as tk\n",
    "import numpy as np\n",
    "from tkinter import filedialog, messagebox, Checkbutton, IntVar\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import time \n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "\n",
    "def resource_path(relative_path):\n",
    "    \"\"\"Get absolute path to resource, works for dev and for PyInstaller exe\"\"\"\n",
    "    try:\n",
    "        if getattr(sys, 'frozen', False):  # If running as .exe\n",
    "            base_path = Path(sys.executable).parent\n",
    "        else:  # If running in a development environment\n",
    "            base_path = Path(__file__).parent\n",
    "    except Exception as e:\n",
    "        base_path = Path(os.path.abspath(os.path.dirname(__file__)))\n",
    "\n",
    "    return base_path / relative_path\n",
    "\n",
    "    \n",
    "def run_selenium_automation(study_name,programmer_name):\n",
    "\n",
    "    timeout = 10\n",
    "    end_time = time.time() + timeout\n",
    "    # Initialize the WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    # options = Options()\n",
    "    # options.add_experimental_option(\"detach\", True) \n",
    "    # options.add_argument(\"--headless\")\n",
    "    # options.add_argument('--no-sandbox')\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    # driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # Replace 'your_pinnacle_21_link' with the actual URL\n",
    "    driver.get('https://pfizer.pinnacle21.net/login?returnPath=%2F')\n",
    "    \n",
    "    try:\n",
    "        # Wait for the \"Log in with Single Sign-On\" button to be clickable and click it\n",
    "        login_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, 'sso_login'))\n",
    "        )\n",
    "        login_button.click()\n",
    "        # time.sleep(10)\n",
    "        # Wait for the table with studies to be visible\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"your-data-packages\"]'))\n",
    "        )\n",
    "        # time.sleep(10)\n",
    "        # Click on the link with the text \"C5241016_SDTM\"\n",
    "        WebDriverWait(driver, 40).until(\n",
    "            EC.element_to_be_clickable((By.LINK_TEXT, study_name))\n",
    "        ).click()\n",
    "        time.sleep(10)\n",
    "        # Wait for the \"View Issues\" button to be clickable and click it\n",
    "        view_issues_button = WebDriverWait(driver, 60).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//a[@class=\"btn btn-link widget-title-link\" and text()=\"View Issues\"]'))\n",
    "        )\n",
    "        view_issues_button.click()\n",
    "        time.sleep(10)\n",
    "            # Wait for the first issue to be present and click on it\n",
    "        first_issue = WebDriverWait(driver, 100).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div[2]/div[2]/div[2]/div[2]/div[1]/div[1]/div/div[2]/div[1]/div[2]/table/tbody/tr[2]/td[3]'))\n",
    "        )\n",
    "        first_issue.click()\n",
    "        # Load the Excel file with the corpus\n",
    "        corpus_path = resource_path(\"CORPUS_P219.xlsx\")\n",
    "        corpus_df = pd.read_excel(corpus_path)\n",
    "        # corpus_df = pd.read_excel(r'C:\\Users\\SRIVAA86\\OneDrive - Pfizer\\Desktop\\P21_AUTO\\CORPUS_P219.xlsx')  # Replace with the actual path to your Excel file\n",
    "        time.sleep(10)\n",
    "        while True:\n",
    "            try:\n",
    "                # Extract the title\n",
    "                title = WebDriverWait(driver, 60).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[1]/h2'))\n",
    "                ).text.strip()\n",
    "                time.sleep(10)\n",
    "                # Navigate to the area with the description\n",
    "                description_area = WebDriverWait(driver, 40).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[1]/aside[1]'))\n",
    "                )\n",
    "                # Extract the description\n",
    "                description = description_area.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[1]/aside[1]/div[1]').text\n",
    "                time.sleep(10)\n",
    "                dataset_area = WebDriverWait(driver, 60).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[1]/aside[2]'))\n",
    "                )\n",
    "                # Extract the dataset name\n",
    "                dataset = dataset_area.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[1]/aside[2]/div[1]')\n",
    "                dataset_text = dataset.text.strip()  # Strip leading and trailing spaces\n",
    "    \n",
    "                # Remove the word \"Dataset\" if it exists in the text\n",
    "                if \"Dataset\" in dataset_text:\n",
    "                    dataset_text = dataset_text.replace(\"Dataset\", \"\").strip()  # Replace and strip again\n",
    "                time.sleep(10)\n",
    "                # Navigate to the bottom panel area\n",
    "                bottom_panel = WebDriverWait(driver, 80).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[3]'))\n",
    "                )\n",
    "                # Extract the table header\n",
    "                table_header = bottom_panel.find_element(By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[3]/div/div[2]').text\n",
    "                time.sleep(10)\n",
    "                # Filter out numbers and specific words from the table header\n",
    "                # filtered_table_header = ' '.join([word for word in table_header.split() \n",
    "                #                                   if not re.match(r'\\d+', word) and word.lower() not in ['variable','dataset','cmdosu','total','cmroute','order', 'length','tsparmcd','lbtestcd','failures','%','affected','records','lbtest','lborresu','egstresu','epoch','ecdosu','egorresu','egtest','egtestcd','exdosu','lbstresu','<','pcspccnd','vsorresu','vsstresu','vstest','vstestcd','qscat']])\n",
    "    \n",
    "                #####################################################################\n",
    "                if 'TREATMENT' in table_header.upper():\n",
    "                # Chunk 1: Logic for handling TREATMENT with single-digit numbers\n",
    "                    exclude_keywords = [\n",
    "                        'variable', 'dataset', 'cmdosu', 'total', 'cmroute', 'order', 'length', 'tsparmcd', 'lbtestcd',\n",
    "                        'failures', '%', 'affected', 'records', 'lbtest', 'lborresu', 'egstresu', 'epoch', 'ecdosu',\n",
    "                        'egorresu', 'egtest', 'egtestcd', 'exdosu', 'lbstresu', '<', 'pcspccnd', 'vsorresu', 'vsstresu',\n",
    "                        'vstest', 'vstestcd', 'qscat','aeout'\n",
    "                    ]\n",
    "                    keywords_allowing_digits = ['treatment']\n",
    "                    \n",
    "                    filtered_table_header = []\n",
    "                    words = table_header.split()\n",
    "                    \n",
    "                    i = 0\n",
    "                    while i < len(words):\n",
    "                        word = words[i].lower()\n",
    "                        if word in exclude_keywords:\n",
    "                            i += 1\n",
    "                            continue\n",
    "                        if word in keywords_allowing_digits and i + 1 < len(words):\n",
    "                            next_word = words[i + 1]\n",
    "                            if re.match(r'^\\d$', next_word):  # Check if the next word is a single digit\n",
    "                                filtered_table_header.append(f\"{words[i].upper()} {next_word}\")\n",
    "                                i += 1  # Skip the next word since it's already added\n",
    "                        i += 1\n",
    "                    \n",
    "                    filtered_table_header = ' '.join(filtered_table_header)\n",
    "                else:\n",
    "                # Chunk 2: Generic filtering logic\n",
    "                    filtered_table_header = ' '.join([\n",
    "                        word for word in table_header.split()\n",
    "                        if not re.match(r'\\d+', word) and word.lower() not in [\n",
    "                            'variable', 'dataset', 'cmdosu', 'total', 'cmroute', 'order', 'length', 'tsparmcd', 'lbtestcd',\n",
    "                            'failures', '%', 'affected', 'records', 'lbtest', 'lborresu', 'egstresu', 'epoch', 'ecdosu',\n",
    "                            'egorresu', 'egtest', 'egtestcd', 'exdosu', 'lbstresu', '<', 'pcspccnd', 'vsorresu', 'vsstresu',\n",
    "                            'vstest', 'vstestcd', 'qscat','aeout'\n",
    "                        ]\n",
    "                    ])\n",
    "                ##################################################################################\n",
    "                print(\"Title:\", title)\n",
    "                print(\"Description:\", description)\n",
    "                print(\"Dataset:\",dataset_text)\n",
    "                print(\"Filtered Table Header:\", filtered_table_header)\n",
    "                \n",
    "                # Find the matching row based on title and description\n",
    "                matching_row = corpus_df[(corpus_df['ID'] == title)] # & (corpus_df['Description'] == description)]\n",
    "                if not matching_row.empty:\n",
    "                    print(\"Matching issue found in corpus, proceeding to Explanation section.\")\n",
    "                                # Navigate to the area containing the explanation button\n",
    "                    explanation_area = WebDriverWait(driver, 50).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul'))\n",
    "                    )\n",
    "                    time.sleep(10)\n",
    "                    # Click on the \"Explanation\" button\n",
    "                    explanation_button = WebDriverWait(driver, 50).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul/li[2]'))\n",
    "                    )\n",
    "                    explanation_button.click()\n",
    "                    time.sleep(10)\n",
    "                # print(matching_row)\n",
    "                ##################################################adding codes to skip to click on save button when explanation is already added#######################\n",
    "                    explain_area = WebDriverWait(driver,50).until(\n",
    "                        EC.presence_of_element_located((By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[2]/div/div[1]/div/div[1]/div[2]/div/div/div'))\n",
    "                    )\n",
    "                    explain_text = explain_area.find_element(By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[2]/div/div[1]/div/div[1]/div[2]/div/div/div/div/div').text\n",
    "                    print(explain_text)\n",
    "                    if explain_text =='':\n",
    "                #######################################################################################################################################\n",
    "                        def format_filtered_table_header(header):\n",
    "                            if \"TREATMENT\" in header:\n",
    "                                treatment_groups=re.findall(r'TREATMENT \\d+',header)\n",
    "                                return \" • \".join(treatment_groups)\n",
    "                            else:\n",
    "                                return \" • \".join(header.split())\n",
    "    \n",
    "                        formatted_header = format_filtered_table_header(filtered_table_header)\n",
    "                        print(formatted_header)\n",
    "                        explanation_text = (matching_row.iloc[0, 3].replace('@dataset', dataset_text) .replace('@variable', formatted_header) .replace('@unit', formatted_header))\n",
    "                                                            \n",
    "                        # Navigate to the area containing the explanation input\n",
    "                        explanation_input = WebDriverWait(driver, 100).until(\n",
    "                            EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[2]/div/div[1]/div/div[1]/div[2]/div/div[2]/div/div/div/div'))\n",
    "                        )\n",
    "                        explanation_input.send_keys(explanation_text)\n",
    "                        time.sleep(10)\n",
    "        \n",
    "                        save_button = WebDriverWait(driver, 100).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[2]/div/div[1]/div/div[2]/button[1]'))\n",
    "                        )\n",
    "                        save_button.click()\n",
    "    \n",
    "                                    # Navigate to the area with the next button\n",
    "                        next_button_area = WebDriverWait(driver, 50).until(\n",
    "                            EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul'))\n",
    "                        )\n",
    "                        time.sleep(5)\n",
    "                        # Click on the issue details tab \n",
    "                        issue_details = WebDriverWait(driver, 50).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul/li[1]'))\n",
    "                        )\n",
    "                        issue_details.click()\n",
    "                        time.sleep(10)\n",
    "    \n",
    "                        manage_area = WebDriverWait(driver,80).until(\n",
    "                            EC.presence_of_element_located((By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]'))\n",
    "                        )\n",
    "                        #Clicking on the status drop down menu\n",
    "                        drop_down = WebDriverWait(driver,80).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[1]/div[1]/div/div'))\n",
    "                        )\n",
    "                        drop_down.click()\n",
    "                        time.sleep(10)\n",
    "                        \n",
    "                        closed_option = WebDriverWait(driver, 80).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//div[text()='Closed']\"))\n",
    "                        )\n",
    "                        closed_option.click() \n",
    "                        #naviagte to the manage issues area again #######################################\n",
    "                        manage_area = WebDriverWait(driver,80).until(\n",
    "                            EC.presence_of_element_located((By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]'))\n",
    "                        )\n",
    "                        #edited the code here ###############################################as the assignee name is not coming\n",
    "                        #Clicking on the status drop down menu\n",
    "                        drop_down2 = WebDriverWait(driver,80).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH,'/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[1]/div[2]/div/div/div[1]'))\n",
    "                        )\n",
    "                        drop_down2.click()\n",
    "                        time.sleep(10)\n",
    "                        # print(manage_area.get_attribute('outerHTML'))\n",
    "                        srivastava_option = WebDriverWait(driver, 80).until(\n",
    "                                EC.element_to_be_clickable((By.XPATH, f\"//div[contains(@class, 'react-select__option') and text()='{programmer_name}']\"))\n",
    "                        )\n",
    "                        srivastava_option.click()\n",
    "                        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", manage_area)\n",
    "                        # print(\"yes\")\n",
    "                    else:\n",
    "                        print(\"Explanation text is already present, skipping to the next button.\")\n",
    "                else:\n",
    "                    print(\"No matching explanation found in the corpus.\")\n",
    "                    manage_area = WebDriverWait(driver, 80).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]'))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", manage_area)\n",
    "                    # if title==\"SD0060: Variable in dataset is not present in define.xml\" or \"user-defined codelist\" in title():\n",
    "                    if \"define.xml\" in title.lower() or \"user-defined codelist\" in title.lower() or \"sd1015: epoch is not in\" in title.lower():\n",
    "                        # Wait for the comments area to load\n",
    "                        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", manage_area)\n",
    "                        comments_area = WebDriverWait(driver, 80).until(\n",
    "                            EC.presence_of_element_located((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div\"))\n",
    "                        )\n",
    "                        time.sleep(10)  # Optional, consider replacing with an explicit wait if necessary\n",
    "                        \n",
    "                        # Determine the comment based on the title\n",
    "                        \n",
    "                        ######commented for C5171002 ############################################3\n",
    "                        if \"define.xml\" in title.lower() or \"user-defined codelist\" in title.lower():\n",
    "                            comment_text = \"You have to make the changes in define.xml\"\n",
    "                        elif \"sd1015: epoch is not in\" in title.lower():\n",
    "                            comment_text = \"You have to edit the Trial Designs.xlsx.\"\n",
    "                        \n",
    "                        # Click on the specific comment section\n",
    "                        comment_section = WebDriverWait(driver, 80).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div/div[2]/div/div[1]/div\"))\n",
    "                        )\n",
    "                        comment_section.click()\n",
    "                    \n",
    "                        # Add the dynamic comment to the comment input field\n",
    "                        comment_input = WebDriverWait(driver, 80).until(\n",
    "                            EC.presence_of_element_located((By.XPATH, \"//textarea[contains(@class, 'comment-editor__input')]\"))  # Update the XPath to match the actual input field\n",
    "                        )\n",
    "                        comment_input.clear()  # Clear any existing text, if necessary\n",
    "                        comment_input.send_keys(comment_text)\n",
    "                    \n",
    "                        # Click on the save button\n",
    "                        time.sleep(10)\n",
    "                        save_button = WebDriverWait(driver, 80).until(\n",
    "                            EC.element_to_be_clickable((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div/div[2]/div/div[2]/button[1]\"))\n",
    "                        )\n",
    "                        save_button.click()\n",
    "    \n",
    "                ##############################################################################\n",
    "                ##############################################################################\n",
    "                    # if \"define.xml\" in title.lower() or \"user-defined codelist\" in title.lower() or \"sd1015: epoch is not in\" in title.lower():\n",
    "                    # # Wait for the comments area to load\n",
    "                    #     driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", manage_area)\n",
    "                    #     comments_area = WebDriverWait(driver, 80).until(\n",
    "                    #         EC.presence_of_element_located((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div\"))\n",
    "                    #     )\n",
    "                    #     time.sleep(10)  # Optional, consider replacing with an explicit wait if necessary\n",
    "                    #     # print(1)\n",
    "                    #     # Click on the specific comment section\n",
    "                    #     comment_section = WebDriverWait(driver, 80).until(\n",
    "                    #         EC.element_to_be_clickable((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div/div[2]/div/div[1]/div\"))\n",
    "                    #     )\n",
    "                    #     comment_section.click()\n",
    "                    #     # print(2)\n",
    "                    #     # Add the text to the comment input field\n",
    "                    #     comment_input = WebDriverWait(driver, 80).until(\n",
    "                    #         EC.presence_of_element_located((By.XPATH, \"//textarea[contains(@class, 'comment-editor__input')]\"))  # Update the XPath to match the actual input field\n",
    "                    #     )\n",
    "                    #     comment_input.clear()  # Clear any existing text, if necessary\n",
    "                    #     comment_input.send_keys(\"You have to edit the define.xml\")\n",
    "                    #     # Click on the save button\n",
    "                    #     time.sleep(10)\n",
    "                    #     save_button = WebDriverWait(driver, 80).until(\n",
    "                    #         EC.element_to_be_clickable((By.XPATH, \"/html/body/div[5]/div/div/div/div[1]/div[1]/div/div[1]/div/section[2]/aside[2]/div/div[2]/div/div[2]/button[1]\"))\n",
    "                    #     )\n",
    "                    #     save_button.click()\n",
    "                    #     # print(1)\n",
    "                    else:\n",
    "                        pass\n",
    "                # else:\n",
    "                #     print(\"Explanation text is not empty, proceeding to the next button.\")\n",
    "                \n",
    "                # Wait for the save operation to complete\n",
    "                time.sleep(10)\n",
    "                # print(2)\n",
    "                # Navigate to the area with the next button\n",
    "                next_button_area = WebDriverWait(driver, 50).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul'))\n",
    "                )\n",
    "                time.sleep(5)\n",
    "                # Click on the issue details tab \n",
    "                issue_details = WebDriverWait(driver, 50).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul/li[1]'))\n",
    "                )\n",
    "                issue_details.click()\n",
    "                time.sleep(10)\n",
    "                \n",
    "     \n",
    "                # Click on the button with the specified XPath to go to the next issue\n",
    "                next_button = WebDriverWait(driver, 50).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div/div/div[1]/div[1]/div/ul/div/span[3]'))\n",
    "                )\n",
    "                next_button.click()\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                print(\"No more issues to process or element not found.\")\n",
    "                break\n",
    "            except TimeoutException as e:\n",
    "                print(f\"Timeout while waiting for an element: {e}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        # driver.quit()\n",
    "        pass\n",
    "\n",
    "\n",
    "# Global variables to store file paths\n",
    "excel_1_path = \"\"  # Reference Excel file path\n",
    "excel_2_path = \"\"  # Study Define Excel file path\n",
    "#FOR COLOR \n",
    "initial_excel_2_copy = None\n",
    "import shutil\n",
    "# Function to upload both Excel files (using the existing function)\n",
    "def upload_excel_files(file_type):\n",
    "    global excel_1_path, excel_2_path, initial_excel_2_copy\n",
    "    try:\n",
    "        if file_type == 'reference':\n",
    "            excel_1_path = filedialog.askopenfilename(title=\"Select Reference Excel File\", filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")])\n",
    "            if excel_1_path:\n",
    "                ref_file_label.config(text=f\"Reference File: {os.path.basename(excel_1_path)}\")\n",
    "                messagebox.showinfo(\"Success\", \"Reference Excel file uploaded successfully!\")\n",
    "                check_all_files_uploaded()  # Check if both files are uploaded\n",
    "        elif file_type == 'study_define':\n",
    "            excel_2_path = filedialog.askopenfilename(title=\"Select Study Define Excel File\", filetypes=[(\"Excel Files\", \"*.xlsx;*.xls\")])\n",
    "            if excel_2_path:\n",
    "                study_file_label.config(text=f\"Study Define File: {os.path.basename(excel_2_path)}\")\n",
    "                messagebox.showinfo(\"Success\", \"Study Define Excel file uploaded successfully!\")\n",
    "                initial_excel_2_copy = f\"initial_{os.path.basename(excel_2_path)}\"\n",
    "                shutil.copy(excel_2_path,initial_excel_2_copy)\n",
    "                check_all_files_uploaded()  # Check if both files are uploaded\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while uploading the file: {e}\")\n",
    "\n",
    "\n",
    "#########################################FOR 3RD TAB###############################\n",
    "issues_excel_path =\"\"\n",
    "study_define_path = \"\"\n",
    "def upload_file(file_type):\n",
    "    global issues_excel_path, study_define_path\n",
    "    try:\n",
    "        # Open the file dialog to select the file\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx;*.xls\"), (\"CSV files\", \"*.csv\")])\n",
    "        \n",
    "        if file_path:\n",
    "            # Update the label based on the file type\n",
    "            if file_type == \"issues\":\n",
    "                issues_excel_path = file_path\n",
    "                issues_excel_file_label.config(text=f\"Issues Excel: {file_path.split('/')[-1]}\")\n",
    "            elif file_type == \"study_define\":\n",
    "                study_define_path = file_path\n",
    "                study_define_file_label.config(text=f\"Study Define Excel: {file_path.split('/')[-1]}\")\n",
    "            \n",
    "            # Enable the start button only if both files are uploaded\n",
    "            if issues_excel_path and study_define_path:\n",
    "                start_button.config(state=tk.NORMAL)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No file selected.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while uploading the file: {e}\")\n",
    "\n",
    "# Global variables\n",
    "ref_file_path = None\n",
    "selected_datasets = []\n",
    "\n",
    "# Function to upload the reference Excel file and extract unique datasets\n",
    "def upload_reference_file():\n",
    "    global ref_file_path, selected_datasets\n",
    "    \n",
    "    try:\n",
    "        # Open file dialog to select the reference Excel file\n",
    "        ref_file_path = filedialog.askopenfilename(filetypes=[(\"Excel files\", \"*.xlsx;*.xls\")])\n",
    "        \n",
    "        if ref_file_path:\n",
    "            ref_file_label.config(text=f\"Reference File: {ref_file_path.split('/')[-1]}\")  # Update label with file name\n",
    "\n",
    "            # # Clear any previous checkboxes and dataset selections\n",
    "            # for widget in datasets_frame.winfo_children():\n",
    "            #     widget.destroy()\n",
    "            dataset_listbox.delete(0,tk.END)\n",
    "            \n",
    "            try:\n",
    "                # Read the Datasets sheet from the uploaded reference Excel file\n",
    "                reference_df = pd.read_excel(ref_file_path, sheet_name=\"Datasets\")\n",
    "                \n",
    "                if \"Dataset\" in reference_df.columns:\n",
    "                    unique_datasets = reference_df[\"Dataset\"].unique()\n",
    "\n",
    "                    # Clear the list of selected datasets\n",
    "                    # selected_datasets = []\n",
    "\n",
    "                    # Populate the frame with checkboxes for each unique dataset\n",
    "                    for dataset in unique_datasets:\n",
    "                        dataset_listbox.insert(tk.END,dataset)\n",
    "\n",
    "                    # Update the UI with the datasets frame\n",
    "                    messagebox.showinfo(\"Success\", \"Reference file uploaded successfully. Please select datasets.\")\n",
    "                else:\n",
    "                    messagebox.showerror(\"Error\", \"The 'Datasets' sheet is missing the 'Dataset' column.\")\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred while reading the reference file: {e}\")\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"No file selected.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while uploading the file: {e}\")\n",
    "\n",
    "\n",
    "# Function to download the selected datasets and update the Excel file\n",
    "def download_selected_datasets():\n",
    "    global ref_file_path, selected_datasets\n",
    "    \n",
    "    try:\n",
    "        # Check if a reference file has been uploaded\n",
    "        if not ref_file_path:\n",
    "            messagebox.showerror(\"Error\", \"Please upload the reference file first.\")\n",
    "            return\n",
    "\n",
    "        # Filter the selected datasets based on checkboxes\n",
    "        # selected_dataset_names = [dataset[0] for dataset in selected_datasets if dataset[1].get() == 1]\n",
    "        selected_dataset_names = [dataset_listbox.get(i) for i in dataset_listbox.curselection()]\n",
    "\n",
    "        if not selected_dataset_names:\n",
    "            messagebox.showerror(\"Error\", \"Please select at least one dataset.\")\n",
    "            return\n",
    "        \n",
    "        # Read the Datasets sheet from the uploaded reference Excel file\n",
    "        reference_df = pd.read_excel(ref_file_path, sheet_name=\"Datasets\")\n",
    "        \n",
    "        # Filter the Datasets sheet to only include selected datasets\n",
    "        filtered_reference_df = reference_df[reference_df[\"Dataset\"].isin(selected_dataset_names)]\n",
    "\n",
    "        # Read the other sheets that need to be filtered (Variables, Codelists, Methods, Comments)\n",
    "        variables_df = pd.read_excel(ref_file_path, sheet_name=\"Variables\")\n",
    "        codelists_df = pd.read_excel(ref_file_path, sheet_name=\"Codelists\")\n",
    "        methods_df = pd.read_excel(ref_file_path, sheet_name=\"Methods\")\n",
    "        comments_df = pd.read_excel(ref_file_path, sheet_name=\"Comments\")\n",
    "        value_level_df = pd.read_excel(ref_file_path,sheet_name=\"ValueLevel\")\n",
    "\n",
    "         # Filter the Variables sheet based on the selected datasets\n",
    "        filtered_variables_df = variables_df[variables_df[\"Dataset\"].isin(selected_dataset_names)]\n",
    "        #filter the valulevel sheet based on the selected datasets\n",
    "        filtered_valuelevel_df = value_level_df[value_level_df[\"Dataset\"].isin(selected_dataset_names)]\n",
    "        # Store the associated Method, Comment, and Codelist for each selected dataset\n",
    "        selected_methods = filtered_variables_df[\"Method\"].dropna().unique().tolist()\n",
    "        selected_comments = filtered_variables_df[\"Comment\"].dropna().unique().tolist()\n",
    "        selected_codelists = filtered_variables_df[\"Codelist\"].dropna().unique().tolist()\n",
    "        # print(selected_methods)\n",
    "        # print(selected_comments)\n",
    "        # Store the associated Method, Comment, and Codelist for each selected dataset in value level\n",
    "        selected_methods_s = filtered_valuelevel_df[\"Method\"].dropna().unique().tolist()\n",
    "        selected_comments_s = filtered_valuelevel_df[\"Comment\"].dropna().unique().tolist()\n",
    "        selected_codelists_s = filtered_valuelevel_df[\"Codelist\"].dropna().unique().tolist()\n",
    "        \n",
    "        # Now filter the Methods, Comments, and Codelists sheets based on the selected datasets (using ID column)\n",
    "        filtered_methods_df = methods_df[methods_df[\"ID\"].isin(selected_methods)]\n",
    "        filtered_comments_df = comments_df[comments_df[\"ID\"].isin(selected_comments)]\n",
    "        filtered_codelists_df = codelists_df[codelists_df[\"ID\"].isin(selected_codelists)]\n",
    "\n",
    "        # Now filter the Methods, Comments, and Codelists sheets based on the selected datasets (using ID column)\n",
    "        filtered_methods_df_s = methods_df[methods_df[\"ID\"].isin(selected_methods_s)]\n",
    "        filtered_comments_df_s = comments_df[comments_df[\"ID\"].isin(selected_comments_s)]\n",
    "        filtered_codelists_df_s = codelists_df[codelists_df[\"ID\"].isin(selected_codelists_s)]\n",
    "\n",
    "                 # COMBINE METHODS & COMMENTS and valuelevel from variables and value level sheet\n",
    "        # ==============================\n",
    "        combined_methods_df = pd.concat([filtered_methods_df, filtered_methods_df_s], ignore_index=True)\n",
    "        combined_methods_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "    \n",
    "        combined_comments_df = pd.concat([filtered_comments_df, filtered_comments_df_s], ignore_index=True)\n",
    "        combined_comments_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "\n",
    "        combined_codelists_df = pd.concat([filtered_codelists_df, filtered_codelists_df], ignore_index=True)\n",
    "        combined_codelists_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "        \n",
    "         # Retain the unmodified sheets\n",
    "        other_sheets = ['Define', 'ValueLevel', 'Dictionaries', 'Documents']  # Add the sheets that should not be changed\n",
    "        all_sheets = pd.read_excel(ref_file_path, sheet_name=None)  # Read all sheets\n",
    "\n",
    "        # Update only the modified sheets\n",
    "        with pd.ExcelWriter(ref_file_path, engine='openpyxl') as writer:\n",
    "            # Write the filtered data to the existing sheets\n",
    "            filtered_reference_df.to_excel(writer, sheet_name=\"Datasets\", index=False)\n",
    "            filtered_variables_df.to_excel(writer, sheet_name=\"Variables\", index=False)\n",
    "            combined_codelists_df.to_excel(writer, sheet_name=\"Codelists\", index=False)\n",
    "            combined_methods_df.to_excel(writer, sheet_name=\"Methods\", index=False)\n",
    "            combined_comments_df.to_excel(writer, sheet_name=\"Comments\", index=False)\n",
    "            filtered_valuelevel_df.to_excel(writer, sheet_name=\"ValueLevel\", index=False)\n",
    "            \n",
    "            # Write unmodified sheets back to the Excel file\n",
    "            for sheet_name in other_sheets:\n",
    "                if sheet_name in all_sheets:\n",
    "                    all_sheets[sheet_name].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        messagebox.showinfo(\"Success\", f\"Selected datasets have been saved back to: {ref_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred while downloading the datasets: {e}\")\n",
    "\n",
    "\n",
    "# Function to check if both files are uploaded\n",
    "def check_all_files_uploaded():\n",
    "    if excel_1_path and excel_2_path:\n",
    "        start_button.config(state=tk.NORMAL)  # Enable the 'Start Process' button\n",
    "\n",
    "\n",
    "def highlighted_updated_cells():\n",
    "    updated_excel_2 = load_workbook(excel_2_path)\n",
    "    initial_excel_2 = load_workbook(initial_excel_2_copy)\n",
    "    yellow_fill = PatternFill(start_color=\"FFFF00\",end_color=\"FFFF00\",fill_type=\"solid\")\n",
    "    #compare all sheets in the initial and updated excel files\n",
    "    for sheet_name in updated_excel_2.sheetnames:\n",
    "        initial_sheet = initial_excel_2[sheet_name]\n",
    "        updated_sheet = updated_excel_2[sheet_name]\n",
    "\n",
    "        for row in updated_sheet.iter_rows(min_row=2,min_col=1,max_row=updated_sheet.max_row,max_col=updated_sheet.max_column):\n",
    "            for cell in row:\n",
    "                initial_cell = initial_sheet.cell(row=cell.row,column=cell.column)\n",
    "                if initial_cell.value != cell.value:\n",
    "                    cell.fill = yellow_fill\n",
    "\n",
    "    updated_excel_2.save(excel_2_path)\n",
    "    print(\"cells updated and highlighted in yellow\")\n",
    "\n",
    "                \n",
    "\n",
    "def start_backend_process():\n",
    "    # Check if both Excel files are uploaded\n",
    "    if not excel_1_path or not excel_2_path:\n",
    "        messagebox.showerror(\"Error\", \"Please upload both Excel files first.\")\n",
    "        return\n",
    "\n",
    "    # Get dataset to delete from entry field\n",
    "    dataset_to_delete = dataset_delete_entry.get().strip()\n",
    "\n",
    "    #################22oct2025##############\n",
    "    dataset_type = dataset_type_dropdown.get()\n",
    "    terminology_version = terminology_version_dropdown.get()\n",
    "    standard_version = standard_version_dropdown.get()\n",
    "\n",
    "    if dataset_type not in ['ADAM','SDTM']:\n",
    "        messagebox.showerror(\"Error\", \"Please select a valid dataset type (ADAM or SDTM).\")\n",
    "        return\n",
    "\n",
    "    if not terminology_version:\n",
    "        messagebox.showerror(\"Error, Please select a terminology version\")\n",
    "        return\n",
    "\n",
    "    if not standard_version:\n",
    "        messagebox.showerror(\"Error, Please select a terminology version\")\n",
    "        return\n",
    "\n",
    "        ################\n",
    "\n",
    "    try:\n",
    "        # Call backend function with all parameters\n",
    "        edit_excel_files(excel_1_path, excel_2_path,dataset_type,terminology_version, dataset_to_delete,standard_version)\n",
    "        highlighted_updated_cells()\n",
    "        messagebox.showinfo(\"Success\", \"Backend process completed successfully.\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred:\\n{str(e)}\")\n",
    "\n",
    "def edit_excel_files(excel_1_path,excel_2_path,dataset_type,terminology_version,dataset_to_delete=None,standard_version=None):\n",
    "    excel_1_path = excel_1_path\n",
    "    sheet_1_name_variables = \"Variables\"\n",
    "    sheet_1_name_methods = \"Methods\"\n",
    "    sheet_1_name_comments = \"Comments\"\n",
    "    #########new addition for value level #########\n",
    "    sheet_1_name_valuelevel=\"ValueLevel\"\n",
    "    ############################\n",
    "    \n",
    "    \n",
    "    excel_2_path = excel_2_path\n",
    "    sheet_2_name_variables = \"Variables\"\n",
    "    #########new addition for value level #########\n",
    "    sheet_2_name_valuelevel=\"ValueLevel\"\n",
    "    ############################\n",
    "\n",
    "    terminology_file = None\n",
    "    if dataset_type==\"ADAM\":\n",
    "        ###start of terminology section for adam part#\n",
    "        if terminology_version == \"ADAM 2025-09-26\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\ADaM_CT_2025-09-26.xlsx\"\n",
    "        elif terminology_version == \"ADAM 2025-03-28\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\ADaM_CT_2025-03-28.xlsx\"\n",
    "        elif terminology_version == \"ADAM 2024-09-27\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\ADaM_CT_2024-09-27.xlsx\"\n",
    "        else:\n",
    "            terminology_file = None\n",
    "\n",
    "        #########to add sdtm/adam version in the datasets sheet\n",
    "        try:\n",
    "            study_df = pd.read_excel(excel_2_path,sheet_name=\"Datasets\")\n",
    "\n",
    "            if standard_version:\n",
    "                study_df['Standard'] = standard_version\n",
    "\n",
    "            # with pd.ExcelWriter(excel_2_path,engine='openpyxl',mode='a') as writer:\n",
    "            #     study_df.to_excel(writer,sheet_name=\"Datasets\",index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\",f\"An error occured while editing the excel file: {e}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            terminology_df = pd.read_excel(terminology_file,sheet_name=pd.ExcelFile(terminology_file).sheet_names[1])\n",
    "            df3_variables = pd.read_excel(excel_2_path,sheet_name=\"Codelists\")\n",
    "\n",
    "            terminology_df['Code'] = terminology_df['Code'].astype(str)\n",
    "            # df3_variables['NCI Term Code'] = df3_variables['NCI Term Code'].astype(str)\n",
    "            df3_variables['NCI Codelist Code'] = df3_variables['NCI Codelist Code'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "            # Update 'Terminology' in df3_variables based on matching codes\n",
    "            for index, row in terminology_df.iterrows():\n",
    "                matching_code = row['Code']\n",
    "                matching_rows = df3_variables[df3_variables['NCI Codelist Code'] == matching_code]\n",
    "    \n",
    "            if not matching_rows.empty:\n",
    "                df3_variables.loc[df3_variables[\"NCI Codelist Code\"] == matching_code, 'Terminology'] = terminology_version\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "            return\n",
    "##########end of terminology section for adam part ############\n",
    "        \n",
    "        # Step 1: Read the 'Variables' sheets from both Excel files\n",
    "        df1_variables = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_variables)\n",
    "        df2_variables = pd.read_excel(excel_2_path, sheet_name=sheet_2_name_variables)\n",
    "        \n",
    "        # Merge the DataFrames based on common columns\n",
    "        merge_columns = ['Dataset', 'Variable']\n",
    "        merged_df = pd.merge(\n",
    "            df2_variables,\n",
    "            df1_variables[['Variable', 'Dataset', 'Comment', 'Origin', 'Source', 'Method', 'Predecessor', 'Data Type']],\n",
    "            how='left',\n",
    "            on=merge_columns\n",
    "        )\n",
    "        \n",
    "        # Update columns with values from df1 only if df2 entries are blank\n",
    "        columns_to_update = ['Comment', 'Origin', 'Source', 'Method', 'Predecessor', 'Data Type']\n",
    "        for column in columns_to_update:\n",
    "            merged_df[column] = merged_df[column + '_x'].combine_first(merged_df[column + '_y'])\n",
    "            merged_df.drop(columns=[f'{column}_x', f'{column}_y'], inplace=True)\n",
    "            \n",
    "        # Sort by 'Dataset' and 'Order'\n",
    "        sort_columns = ['Dataset', 'Order']\n",
    "        sorted_df = merged_df.sort_values(by=sort_columns)\n",
    "        \n",
    "        # Replace 0 with 1 in the 'Length' column\n",
    "        sorted_df['Length'].replace(0, 1, inplace=True)\n",
    "        \n",
    "        # Update 'Source' where 'Origin' is 'assigned' or 'derived'\n",
    "        sorted_df['Source'] = np.where(\n",
    "            sorted_df['Origin'].str.lower().isin(['assigned', 'derived']),\n",
    "            'Sponsor',\n",
    "            ''\n",
    "        )\n",
    "        \n",
    "        # Handle 'Has No Data' and 'Comment' columns\n",
    "        sorted_df['Has No Data'].fillna('', inplace=True)\n",
    "        sorted_df['Comment'] = np.where(\n",
    "            sorted_df['Has No Data'].str.strip().str.lower() == 'yes',\n",
    "            sorted_df['Comment'],  # Retain original comment if 'yes'\n",
    "            ''  # Otherwise, set to empty\n",
    "        )\n",
    "        \n",
    "        # Step 2: Filter Methods based on the 'Method' column from the variables sheet\n",
    "        df1_methods = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_methods)\n",
    "        methods_list = sorted_df['Method'].dropna().tolist()\n",
    "        filtered_methods_df = df1_methods[df1_methods['ID'].isin(methods_list)]\n",
    "        # print(filtered_methods_df)\n",
    "        \n",
    "        # Step 3: Filter Comments based on the 'Comment' column from the variables sheet\n",
    "        df1_comments = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_comments)\n",
    "        comments_list = sorted_df['Comment'].dropna().tolist()\n",
    "        filtered_comments_df = df1_comments[df1_comments['ID'].isin(comments_list)]\n",
    "    \n",
    "        ##########################new addition for value level #######################\n",
    "        try:\n",
    "            df1_valuelevel = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_valuelevel)\n",
    "            df2_valuelevel = pd.read_excel(excel_2_path,sheet_name=sheet_2_name_valuelevel)\n",
    "    \n",
    "            #merge based on dataset + variable + where clause variable \n",
    "            merge_columns_v1 = ['Dataset','Variable','Where Clause']\n",
    "            merged_v1_df = pd.merge(\n",
    "                df2_valuelevel,\n",
    "                df1_valuelevel[['Dataset','Variable','Where Clause','Data Type',\n",
    "                                'Length','Origin','Source','Method','Predecessor',\n",
    "                                'Comment'\n",
    "                ]],\n",
    "                how='left',\n",
    "                on=merge_columns_v1\n",
    "            )\n",
    "    \n",
    "            #update only if blank\n",
    "            columns_to_update_v1 = ['Data Type','Length','Origin','Source','Method','Predecessor','Comment']\n",
    "            for column in columns_to_update_v1:\n",
    "                merged_v1_df[column] = merged_v1_df[column + '_x'].combine_first(merged_v1_df[column + '_y'])\n",
    "                merged_v1_df.drop(columns=[f'{column}_x', f'{column}_y'],inplace=True)\n",
    "    \n",
    "            sorted_v1_df = merged_v1_df.sort_values(by=['Dataset', 'Variable', 'Where Clause'])\n",
    "            #replace 0 with 1 in length\n",
    "            sorted_v1_df['Length'].replace(0,1,inplace=True)\n",
    "            # Filter Methods\n",
    "            methods_list_v1 = sorted_v1_df['Method'].dropna().tolist()\n",
    "            filtered_methods_v1_df = df1_methods[df1_methods['ID'].isin(methods_list_v1)]\n",
    "            # print(filtered_methods_v1_df)\n",
    "    \n",
    "            # Filter Comments\n",
    "            comments_list_v1 = sorted_v1_df['Comment'].dropna().tolist()\n",
    "            filtered_comments_v1_df = df1_comments[df1_comments['ID'].isin(comments_list_v1)]\n",
    "            print(\"ValueLevel sheet merged successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"ValueLevel sheet not found in one of the files or error occurred:\", e)\n",
    "            sorted_v1_df = pd.DataFrame() \n",
    "            filtered_methods_v1_df = pd.DataFrame()\n",
    "            filtered_comments_v1_df = pd.DataFrame()\n",
    "    \n",
    "        ############################\n",
    "    \n",
    "         # COMBINE METHODS & COMMENTS from variables and value level sheet\n",
    "        # ==============================\n",
    "        combined_methods_df = pd.concat([filtered_methods_df, filtered_methods_v1_df], ignore_index=True)\n",
    "        combined_methods_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "    \n",
    "        combined_comments_df = pd.concat([filtered_comments_df, filtered_comments_v1_df], ignore_index=True)\n",
    "        combined_comments_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "        #################################################################\n",
    "        \n",
    "        # Load the workbook into memory\n",
    "        wb = load_workbook(excel_2_path)\n",
    "        \n",
    "        # List of sheets you want to remove\n",
    "        sheets_to_delete = ['Datasets','Variables', 'Methods', 'Comments','ValueLevel','Codelists']\n",
    "        \n",
    "        # Loop through each sheet name\n",
    "        for sheet_name in sheets_to_delete:\n",
    "            if sheet_name in wb.sheetnames:  # Check if the sheet exists\n",
    "                del wb[sheet_name]            # Delete the sheet\n",
    "                print(f\"Deleted sheet: {sheet_name}\")\n",
    "            else:\n",
    "                print(f\"Sheet '{sheet_name}' not found, skipping.\")\n",
    "        \n",
    "        # Save the workbook after deletions\n",
    "        wb.save(excel_2_path)\n",
    "        \n",
    "           # Write back combined data\n",
    "        with pd.ExcelWriter(excel_2_path, engine='openpyxl', mode='a') as writer:\n",
    "            sorted_df.to_excel(writer, sheet_name='Variables', index=False)\n",
    "            sorted_v1_df.to_excel(writer, sheet_name='ValueLevel', index=False)\n",
    "            combined_methods_df.to_excel(writer, sheet_name='Methods', index=False)\n",
    "            combined_comments_df.to_excel(writer, sheet_name='Comments', index=False)\n",
    "             #####################\n",
    "            df3_variables.to_excel(writer,sheet_name=\"Codelists\",index=False)\n",
    "            ###################\n",
    "            study_df.to_excel(writer,sheet_name=\"Datasets\",index=False)\n",
    "        \n",
    "    \n",
    "            ##############################################\n",
    "        \n",
    "        print(\"DataFrames written to the Excel file successfully.\")\n",
    "        ###\n",
    "                # ========== DATASET DELETION (SAFE POST-UPDATE) =========\n",
    "        # Split the dataset names by commas and remove any extra spaces around them\n",
    "        domain_to_delete = dataset_to_delete.strip() if dataset_to_delete else \"\"\n",
    "        datasets_to_delete = [dataset.strip() for dataset in domain_to_delete.split(\",\") if dataset.strip()]\n",
    "        \n",
    "        # Check if there are any datasets to delete\n",
    "        if datasets_to_delete:\n",
    "            print(f\"Deleting datasets {datasets_to_delete} from all sheets...\")\n",
    "            \n",
    "            # Read the updated sheets from Excel\n",
    "            df_variables = pd.read_excel(excel_2_path, sheet_name='Variables')\n",
    "            df_methods = pd.read_excel(excel_2_path, sheet_name='Methods')\n",
    "            df_comments = pd.read_excel(excel_2_path, sheet_name='Comments')\n",
    "            df_codelists = pd.read_excel(excel_2_path,sheet_name='Codelists')\n",
    "            df_datasets = pd.read_excel(excel_2_path,sheet_name='Datasets')\n",
    "        \n",
    "            # Loop through each dataset to delete it\n",
    "            for domain_to_delete in datasets_to_delete:\n",
    "                if domain_to_delete:  # Ensure that the dataset name is not empty\n",
    "                    print(f\"Deleting dataset '{domain_to_delete}' from all sheets...\")\n",
    "                    \n",
    "                    # Filter out the dataset from the 'Variables' sheet\n",
    "                    to_delete_df = df_variables[df_variables['Dataset'] == domain_to_delete]\n",
    "                    methods_to_delete = to_delete_df['Method'].dropna().unique().tolist()\n",
    "                    comments_to_delete = to_delete_df['Comment'].dropna().unique().tolist()\n",
    "                    codelists_to_delete = to_delete_df['Codelist'].dropna().unique().tolist()\n",
    "        \n",
    "                    # Remove the dataset from Variables, Methods, and Comments sheets\n",
    "                    df_variables = df_variables[df_variables['Dataset'] != domain_to_delete]\n",
    "                    df_methods = df_methods[~df_methods['ID'].isin(methods_to_delete)]\n",
    "                    df_comments = df_comments[~df_comments['ID'].isin(comments_to_delete)]\n",
    "                    df_codelists = df_codelists[~df_codelists['ID'].isin(codelists_to_delete)]\n",
    "                    df_datasets = df_datasets[df_datasets['Dataset'] != domain_to_delete]\n",
    "                    \n",
    "                    print(f\"✅ Dataset '{domain_to_delete}' deleted successfully.\")\n",
    "        \n",
    "            # Overwrite the final Excel file safely after deletions\n",
    "            wb = load_workbook(excel_2_path)\n",
    "            for sname in ['Variables', 'Methods', 'Comments','Codelists','Datasets']:\n",
    "                if sname in wb.sheetnames:\n",
    "                    del wb[sname]  \n",
    "            wb.save(excel_2_path)\n",
    "        \n",
    "            # Write the updated DataFrames back into the Excel file\n",
    "            with pd.ExcelWriter(excel_2_path, engine='openpyxl', mode='a') as writer:\n",
    "                df_variables.to_excel(writer, sheet_name='Variables', index=False)\n",
    "                df_methods.to_excel(writer, sheet_name='Methods', index=False)\n",
    "                df_comments.to_excel(writer, sheet_name='Comments', index=False)\n",
    "                df_codelists.to_excel(writer,sheet_name='Codelists',index=False)\n",
    "                df_datasets.to_excel(writer,sheet_name='Datasets',index=False)\n",
    "        \n",
    "            print(f\"✅ All specified datasets deleted and Excel file updated successfully.\")\n",
    "        else:\n",
    "            print(\"No datasets specified — skipping deletion step.\")\n",
    "        messagebox.showinfo(\"Success\",\"Excel file updated successfully\")\n",
    "        \n",
    "#########################################################SDTM##########################################################################################\n",
    "    else:\n",
    "        ###start of terminolgy section#################\n",
    "        if terminology_version == \"SDTM 2025-09-26\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\SDTM_CT_2025-09-26.xlsx\"\n",
    "        elif terminology_version == \"SDTM 2025-03-28\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\SDTM_CT_2025-03-28.xlsx\"\n",
    "        elif terminology_version == \"SDTM 2024-09-27\":\n",
    "            terminology_file = r\"C:\\Users\\SRIVAA86\\Downloads\\SDTM_CT_2024-09-27.xlsx\"\n",
    "        else:\n",
    "            terminology_file=None\n",
    "            \n",
    "        try:\n",
    "            study_df = pd.read_excel(excel_2_path,sheet_name=\"Datasets\")\n",
    "\n",
    "            if standard_version:\n",
    "                study_df['Standard'] = standard_version\n",
    "\n",
    "            # with pd.ExcelWriter(excel_2_path,engine='openpyxl',mode='a') as writer:\n",
    "            #     study_df.to_excel(writer,sheet_name=\"Datasets\",index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\",f\"An error occured while editing the excel file: {e}\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            terminology_df = pd.read_excel(terminology_file,sheet_name=pd.ExcelFile(terminology_file).sheet_names[1])\n",
    "            df3_variables = pd.read_excel(excel_2_path,sheet_name=\"Codelists\")\n",
    "\n",
    "            terminology_df['Code'] = terminology_df['Code'].astype(str)\n",
    "            # df3_variables['NCI Term Code'] = df3_variables['NCI Term Code'].astype(str)\n",
    "            df3_variables['NCI Codelist Code'] = df3_variables['NCI Codelist Code'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "            # Update 'Terminology' in df3_variables based on matching codes\n",
    "            for index, row in terminology_df.iterrows():\n",
    "                matching_code = row['Code']\n",
    "                matching_rows = df3_variables[df3_variables['NCI Codelist Code'] == matching_code]\n",
    "    \n",
    "                if not matching_rows.empty:\n",
    "                    df3_variables.loc[df3_variables[\"NCI Codelist Code\"] == matching_code, 'Terminology'] = terminology_version\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "            return\n",
    "\n",
    "########################################end of terminology section ########################################\n",
    "         # Step 1: Read the 'Variables' sheets from both Excel files\n",
    "        df1_variables = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_variables)\n",
    "        df2_variables = pd.read_excel(excel_2_path, sheet_name=sheet_2_name_variables)\n",
    "        \n",
    "        # Merge the DataFrames based on common columns\n",
    "        merge_columns = ['Dataset', 'Variable']\n",
    "        merged_df = pd.merge(\n",
    "            df2_variables,\n",
    "            df1_variables[['Variable', 'Dataset', 'Comment', 'Origin', 'Source', 'Method','Data Type']],\n",
    "            how='left',\n",
    "            on=merge_columns\n",
    "        )\n",
    "        \n",
    "        # Update columns with values from df1 only if df2 entries are blank\n",
    "        columns_to_update = ['Comment', 'Origin', 'Source', 'Method','Data Type']\n",
    "        for column in columns_to_update:\n",
    "            merged_df[column] = merged_df[column + '_x'].combine_first(merged_df[column + '_y'])\n",
    "            merged_df.drop(columns=[f'{column}_x', f'{column}_y'], inplace=True)\n",
    "\n",
    "        # Sort by 'Dataset' and 'Order'\n",
    "        sort_columns = ['Dataset', 'Order']\n",
    "        sorted_df = merged_df.sort_values(by=sort_columns)\n",
    "        \n",
    "        # Replace 0 with 1 in the 'Length' column\n",
    "        sorted_df['Length'].replace(0, 1, inplace=True)\n",
    "        \n",
    "        # Update 'Source' where 'Origin' is 'assigned' or 'derived'\n",
    "        sorted_df['Source'] = np.where(\n",
    "            sorted_df['Origin'].str.lower().isin(['assigned', 'derived']),\n",
    "            'Sponsor',\n",
    "            ''\n",
    "        )\n",
    "        \n",
    "        # Handle 'Has No Data' and 'Comment' columns\n",
    "        sorted_df['Has No Data'].fillna('', inplace=True)\n",
    "        sorted_df['Comment'] = np.where(\n",
    "            sorted_df['Has No Data'].str.strip().str.lower() == 'yes',\n",
    "            sorted_df['Comment'],  # Retain original comment if 'yes'\n",
    "            ''  # Otherwise, set to empty\n",
    "        )\n",
    "        \n",
    "        # Step 2: Filter Methods based on the 'Method' column from the variables sheet\n",
    "        df1_methods = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_methods)\n",
    "        methods_list = sorted_df['Method'].dropna().tolist()\n",
    "        filtered_methods_df = df1_methods[df1_methods['ID'].isin(methods_list)]\n",
    "        # print(filtered_methods_df)\n",
    "        \n",
    "        # Step 3: Filter Comments based on the 'Comment' column from the variables sheet\n",
    "        df1_comments = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_comments)\n",
    "        comments_list = sorted_df['Comment'].dropna().tolist()\n",
    "        filtered_comments_df = df1_comments[df1_comments['ID'].isin(comments_list)]\n",
    "    \n",
    "        ##########################new addition for value level #######################\n",
    "        try:\n",
    "            df1_valuelevel = pd.read_excel(excel_1_path, sheet_name=sheet_1_name_valuelevel)\n",
    "            df2_valuelevel = pd.read_excel(excel_2_path,sheet_name=sheet_2_name_valuelevel)\n",
    "    \n",
    "            #merge based on dataset + variable + where clause variable \n",
    "            merge_columns_v1 = ['Dataset','Variable','Where Clause']\n",
    "            merged_v1_df = pd.merge(\n",
    "                df2_valuelevel,\n",
    "                df1_valuelevel[['Dataset','Variable','Where Clause','Data Type',\n",
    "                                'Length','Origin','Source','Method',\n",
    "                                'Comment'\n",
    "                ]],\n",
    "                how='left',\n",
    "                on=merge_columns_v1\n",
    "            )\n",
    "    \n",
    "            #update only if blank\n",
    "            columns_to_update_v1 = ['Data Type','Length','Origin','Source','Method','Comment']\n",
    "            for column in columns_to_update_v1:\n",
    "                merged_v1_df[column] = merged_v1_df[column + '_x'].combine_first(merged_v1_df[column + '_y'])\n",
    "                merged_v1_df.drop(columns=[f'{column}_x', f'{column}_y'],inplace=True)\n",
    "    \n",
    "            sorted_v1_df = merged_v1_df.sort_values(by=['Dataset', 'Variable', 'Where Clause'])\n",
    "            #replace 0 with 1 in length\n",
    "            sorted_v1_df['Length'].replace(0,1,inplace=True)\n",
    "            # Filter Methods\n",
    "            methods_list_v1 = sorted_v1_df['Method'].dropna().tolist()\n",
    "            filtered_methods_v1_df = df1_methods[df1_methods['ID'].isin(methods_list_v1)]\n",
    "            # print(filtered_methods_v1_df)\n",
    "    \n",
    "            # Filter Comments\n",
    "            comments_list_v1 = sorted_v1_df['Comment'].dropna().tolist()\n",
    "            filtered_comments_v1_df = df1_comments[df1_comments['ID'].isin(comments_list_v1)]\n",
    "            print(\"ValueLevel sheet merged successfully.\")\n",
    "        except Exception as e:\n",
    "            print(\"ValueLevel sheet not found in one of the files or error occurred:\", e)\n",
    "            sorted_v1_df = pd.DataFrame() \n",
    "            filtered_methods_v1_df = pd.DataFrame()\n",
    "            filtered_comments_v1_df = pd.DataFrame()\n",
    "    \n",
    "        ############################\n",
    "    \n",
    "         # COMBINE METHODS & COMMENTS from variables and value level sheet\n",
    "        # ==============================\n",
    "        combined_methods_df = pd.concat([filtered_methods_df, filtered_methods_v1_df], ignore_index=True)\n",
    "        combined_methods_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "    \n",
    "        combined_comments_df = pd.concat([filtered_comments_df, filtered_comments_v1_df], ignore_index=True)\n",
    "        combined_comments_df.drop_duplicates(subset=['ID'], inplace=True)\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        # Load the workbook into memory\n",
    "        wb = load_workbook(excel_2_path)\n",
    "        \n",
    "        # List of sheets you want to remove\n",
    "        sheets_to_delete = ['Datasets','Variables', 'Methods', 'Comments','ValueLevel','Codelists']\n",
    "        \n",
    "        # Loop through each sheet name\n",
    "        for sheet_name in sheets_to_delete:\n",
    "            if sheet_name in wb.sheetnames:  # Check if the sheet exists\n",
    "                del wb[sheet_name]            # Delete the sheet\n",
    "                print(f\"Deleted sheet: {sheet_name}\")\n",
    "            else:\n",
    "                print(f\"Sheet '{sheet_name}' not found, skipping.\")\n",
    "        \n",
    "        # Save the workbook after deletions\n",
    "        wb.save(excel_2_path)\n",
    "        \n",
    "           # Write back combined data\n",
    "        with pd.ExcelWriter(excel_2_path, engine='openpyxl', mode='a') as writer:\n",
    "            sorted_df.to_excel(writer, sheet_name='Variables', index=False)\n",
    "            sorted_v1_df.to_excel(writer, sheet_name='ValueLevel', index=False)\n",
    "            combined_methods_df.to_excel(writer, sheet_name='Methods', index=False)\n",
    "            combined_comments_df.to_excel(writer, sheet_name='Comments', index=False)\n",
    "            #####################\n",
    "            df3_variables.to_excel(writer,sheet_name=\"Codelists\",index=False)\n",
    "            ###################\n",
    "            study_df.to_excel(writer,sheet_name=\"Datasets\",index=False)\n",
    "        \n",
    "    \n",
    "            ##############################################\n",
    "        \n",
    "        print(\"DataFrames written to the Excel file successfully.\")\n",
    "        ###\n",
    "        # ========== DATASET DELETION (SAFE POST-UPDATE) =========\n",
    "        # Split the dataset names by commas and remove any extra spaces around them\n",
    "        domain_to_delete = dataset_to_delete.strip() if dataset_to_delete else \"\"\n",
    "        datasets_to_delete = [dataset.strip() for dataset in domain_to_delete.split(\",\") if dataset.strip()]\n",
    "        \n",
    "        # Check if there are any datasets to delete\n",
    "        if datasets_to_delete:\n",
    "            print(f\"Deleting datasets {datasets_to_delete} from all sheets...\")\n",
    "            \n",
    "            # Read the updated sheets from Excel\n",
    "            df_variables = pd.read_excel(excel_2_path, sheet_name='Variables')\n",
    "            df_methods = pd.read_excel(excel_2_path, sheet_name='Methods')\n",
    "            df_comments = pd.read_excel(excel_2_path, sheet_name='Comments')\n",
    "            df_codelists = pd.read_excel(excel_2_path,sheet_name='Codelists')\n",
    "            df_datasets = pd.read_excel(excel_2_path,sheet_name='Datasets')\n",
    "        \n",
    "            # Loop through each dataset to delete it\n",
    "            for domain_to_delete in datasets_to_delete:\n",
    "                if domain_to_delete:  # Ensure that the dataset name is not empty\n",
    "                    print(f\"Deleting dataset '{domain_to_delete}' from all sheets...\")\n",
    "                    \n",
    "                    # Filter out the dataset from the 'Variables' sheet\n",
    "                    to_delete_df = df_variables[df_variables['Dataset'] == domain_to_delete]\n",
    "                    methods_to_delete = to_delete_df['Method'].dropna().unique().tolist()\n",
    "                    comments_to_delete = to_delete_df['Comment'].dropna().unique().tolist()\n",
    "                    codelists_to_delete = to_delete_df['Codelist'].dropna().unique().tolist()\n",
    "        \n",
    "                    # Remove the dataset from Variables, Methods, and Comments sheets\n",
    "                    df_variables = df_variables[df_variables['Dataset'] != domain_to_delete]\n",
    "                    df_methods = df_methods[~df_methods['ID'].isin(methods_to_delete)]\n",
    "                    df_comments = df_comments[~df_comments['ID'].isin(comments_to_delete)]\n",
    "                    df_codelists = df_codelists[~df_codelists['ID'].isin(codelists_to_delete)]\n",
    "                    df_datasets = df_datasets[df_datasets['Dataset'] != domain_to_delete]\n",
    "                    \n",
    "                    print(f\"✅ Dataset '{domain_to_delete}' deleted successfully.\")\n",
    "        \n",
    "            # Overwrite the final Excel file safely after deletions\n",
    "            wb = load_workbook(excel_2_path)\n",
    "            for sname in ['Variables', 'Methods', 'Comments','Codelists','Datasets']:\n",
    "                if sname in wb.sheetnames:\n",
    "                    del wb[sname]  \n",
    "            wb.save(excel_2_path)\n",
    "        \n",
    "            # Write the updated DataFrames back into the Excel file\n",
    "            with pd.ExcelWriter(excel_2_path, engine='openpyxl', mode='a') as writer:\n",
    "                df_variables.to_excel(writer, sheet_name='Variables', index=False)\n",
    "                df_methods.to_excel(writer, sheet_name='Methods', index=False)\n",
    "                df_comments.to_excel(writer, sheet_name='Comments', index=False)\n",
    "                df_codelists.to_excel(writer,sheet_name='Codelists',index=False)\n",
    "                df_datasets.to_excel(writer,sheet_name='Datasets',index=False)\n",
    "        \n",
    "            print(f\"✅ All specified datasets deleted and Excel file updated successfully.\")\n",
    "        else:\n",
    "            print(\"No datasets specified — skipping deletion step.\")\n",
    "        messagebox.showinfo(\"Success\",\"Excel file updated successfully\")\n",
    "\n",
    "\n",
    "###########################to add the 3rd tab#############################\n",
    "\n",
    "model = BartForSequenceClassification.from_pretrained(r\"C:\\Users\\SRIVAA86\\Downloads\\saved_model\")\n",
    "tokenizer = BartTokenizer.from_pretrained(r\"C:\\Users\\SRIVAA86\\Downloads\\saved_model\")\n",
    "\n",
    "def predict_comment(comment):\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = np.argmax(logits.detach().numpy())\n",
    "    return label_to_description[predicted_label]\n",
    "\n",
    "\n",
    "def start_the_process():\n",
    "    if not issues_excel_path or not study_define_path:\n",
    "        messagebox.showerror(\"Error\",\"Please Upload both Issues Excel and study define excel files\")\n",
    "        return\n",
    "    try:\n",
    "        issues_df = read_issues(issues_excel_path)\n",
    "        fix_issue_in_sheet(study_define_path,issues_df)\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\",f\"An error occured: {e}\")\n",
    "    \n",
    "def read_issues(sheet_path):\n",
    "    try:\n",
    "        issues_df = pd.read_csv(sheet_path)\n",
    "        issues_df[['Variables_A','Variables_B']] = issues_df['Variables'].str.split(',',n=1,expand=True)\n",
    "        issues_df['Variables_A']=issues_df['Variables_A'].fillna('')\n",
    "        issues_df['Variables_B']=issues_df['Variables_B'].fillna('')\n",
    "        issues_df['Variables_B']=issues_df['Variables_B'].apply(lambda x: 'Codelists' if 'CodeList Name' in x else x)\n",
    "\n",
    "        issues_df[['Values_A','Values_B']] = issues_df['Values'].str.split(',',n=1,expand=True)\n",
    "        issues_df['Values_A']=issues_df['Values_A'].fillna('')\n",
    "        issues_df['Values_B']=issues_df['Values_B'].fillna('')\n",
    "        return issues_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading issues: {e}\")\n",
    "        return None\n",
    "\n",
    "def fix_issue_in_sheet(study_define_path,issues_df):\n",
    "    try:\n",
    "        for index, row in issues_df.iterrows():\n",
    "            print(f\"checking row {index}: Variables_B = '{row['Variables_B']}'\")\n",
    "            if \"Codelists\" in row[\"Variables_B\"]:\n",
    "                sheet_name = \"Codelists\"\n",
    "                study_define_df=pd.read_excel(study_define_path,sheet_name=sheet_name)\n",
    "                match_row = study_define_df[study_define_df['ID'] == row['Values_A']]\n",
    "\n",
    "                if not match_row.empty:\n",
    "                    value = row['Values_A'].split('.')[1] if row['Values_A'] else ''\n",
    "                    current_name = match_row.iloc[0]['Name']\n",
    "                    new_name = f\"{current_name} for {value}\" if value else current_name\n",
    "                    print(new_name)\n",
    "                    study_define_df.loc[study_define_df['ID'] == row['Values_A'],'Name'] = new_name\n",
    "    \n",
    "                    with pd.ExcelWriter(study_define_path,engine='openpyxl',mode='a',if_sheet_exists='replace') as writer:\n",
    "                        study_define_df.to_excel(writer,sheet_name=sheet_name,index=False)\n",
    "    \n",
    "                print(\"All issues fixed successfully\")\n",
    "            if row[\"Variables_B\"] == \" Variable\":\n",
    "                sheet_name = \"Variables\"\n",
    "                study_define_df = pd.read_excel(study_define_path,sheet_name=sheet_name)\n",
    "                \n",
    "                #find the row where both values_A(dataset) and values_b(variable) match\n",
    "                match_row = study_define_df[(study_define_df['Dataset'] == row['Values_A'].strip()) & (study_define_df['Variable'] == row['Values_B'].strip())] \n",
    "                print(match_row)\n",
    "                if not match_row.empty:\n",
    "                    comment=f\"{row['Values_A']}.{row['Values_B'].strip()}\"\n",
    "                    print(comment)\n",
    "                    study_define_df.loc[(study_define_df['Dataset'] == row['Values_A'].strip()) & (study_define_df['Variable'] == row['Values_B'].strip()),'Comment'] = comment\n",
    "                    description = predict_comment(comment) #the BART model \n",
    "                    print(description)\n",
    "                    with pd.ExcelWriter(study_define_path,engine='openpyxl',mode='a',if_sheet_exists='replace') as writer:\n",
    "                        study_define_df.to_excel(writer,sheet_name=sheet_name,index=False)\n",
    "\n",
    "                    #now we need to update the comment sheet\n",
    "                    comment_sheet=\"Comments\"\n",
    "                    comment_df = pd.read_excel(study_define_path,sheet_name=comment_sheet)\n",
    "\n",
    "                    new_comment_row = pd.DataFrame({\n",
    "                        'ID' : [comment],\n",
    "                        'Description' : [description]\n",
    "                    })\n",
    "\n",
    "                    # append the new comment row to the comment sheet\n",
    "                    comment_df = pd.concat([comment_df,new_comment_row],ignore_index=True)\n",
    "\n",
    "                    with pd.ExcelWriter(study_define_path,engine='openpyxl',mode='a',if_sheet_exists='replace') as writer:\n",
    "                        comment_df.to_excel(writer,sheet_name=comment_sheet,index=False)\n",
    "\n",
    "            # else:\n",
    "            #     pass\n",
    "        print(\"Issues fixed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fixing issue: {e}\")\n",
    "\n",
    "def create_gui():\n",
    "    global dataset_listbox, ref_file_labe\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Pinnacle 21 Automation Tool\")\n",
    "\n",
    "    # Set the overall font style for the application\n",
    "    app_font = (\"Helvetica\", 12)\n",
    "\n",
    "    # Set the style for ttk widgets\n",
    "    style = ttk.Style()\n",
    "    style.configure(\"TNotebook.Tab\", background=\"#007BFF\", font=(\"Helvetica\", 12, \"bold\"), foreground=\"black\")\n",
    "    style.configure(\"TButton\", font=app_font, padding=10)\n",
    "    style.configure(\"TLabel\", font=app_font, background=\"white\",foreground=\"black\")\n",
    "    style.configure(\"TCombobox\", font=app_font, background=\"white\")\n",
    "\n",
    "    # Header frame for the title\n",
    "    header_frame = tk.Frame(root, bg=\"#007BFF\", height=50)\n",
    "    header_frame.pack(fill=\"x\", padx=0, pady=0)\n",
    "    \n",
    "    header_label = tk.Label(header_frame, text=\"Pinnacle 21 Automation Tool\", font=(\"Helvetica\", 16, \"bold\"), bg=\"#007BFF\", fg=\"white\")\n",
    "    header_label.pack(expand=True, fill=\"both\", padx=20, pady=20)\n",
    "\n",
    "    # Tab control\n",
    "    tab_control = ttk.Notebook(root)\n",
    "    tab_1 = ttk.Frame(tab_control)\n",
    "    tab_control.add(tab_1, text=\"Dataset Selection & Download\")\n",
    "    # tab_4.config(bg=\"white\")\n",
    "\n",
    "    ref_labe = tk.Label(tab_1, text=\"Upload Reference Excel file:\", font=app_font, bg=\"white\")\n",
    "    ref_labe.pack(pady=10)\n",
    "\n",
    "    ref_butto = tk.Button(tab_1, text=\"Upload Reference File\", command=upload_reference_file, font=app_font, bg=\"#007BFF\", fg=\"white\", relief=\"raised\")\n",
    "    ref_butto.pack(pady=10)\n",
    "\n",
    "    global ref_file_labe\n",
    "    ref_file_labe = tk.Label(tab_1, text=\"Reference File: Not uploaded\", font=app_font, bg=\"white\", anchor=\"w\", width=40)\n",
    "    ref_file_labe.pack(pady=10)\n",
    "\n",
    "    dataset_label = tk.Label(tab_1, text=\"Select Datasets:\", font=app_font, bg=\"white\")\n",
    "    dataset_label.pack(pady=10)\n",
    "\n",
    "    dataset_listbox = tk.Listbox(tab_1, selectmode=tk.MULTIPLE, font=app_font, height=10, width=40)\n",
    "    dataset_listbox.pack(pady=10)\n",
    "\n",
    "    download_button = tk.Button(tab_1, text=\"Download Selected Datasets\", command=download_selected_datasets, font=app_font, bg=\"#4CAF50\", fg=\"white\", relief=\"raised\")\n",
    "    download_button.pack(pady=10)\n",
    "    \n",
    "\n",
    "    # Tab 2: Excel Files Upload and Editing\n",
    "    tab_2 = ttk.Frame(tab_control)\n",
    "    tab_control.add(tab_2, text=\"Define MetaData Updates (Excel)\")\n",
    "    # tab_2.config(bg=\"white\")\n",
    "    global dataset_type_dropdown\n",
    "    dataset_type_label = tk.Label(tab_2, text=\"Select Dataset type (SDTM/ADAM):\", font=app_font, bg=\"white\")\n",
    "    dataset_type_label.pack(pady=10)\n",
    "\n",
    "    dataset_type = tk.StringVar()\n",
    "    dataset_type_dropdown = ttk.Combobox(tab_2, textvariable=dataset_type, values=[\"SDTM\", \"ADAM\"], state=\"readonly\", font=(\"Helvetica\", 11), width=20)\n",
    "    dataset_type_dropdown.pack(pady=5)\n",
    "    dataset_type_dropdown.set(\"SDTM\")\n",
    "\n",
    "    terminology_version_label = tk.Label(tab_2, text=\"Select Terminology Version:\", font=app_font, bg=\"white\")\n",
    "    terminology_version_label.pack(pady=5)\n",
    "    \n",
    "    global terminology_version_dropdown\n",
    "    terminology_version_dropdown = ttk.Combobox(tab_2, state=\"readonly\", font=(\"Helvetica\", 9), width=10)\n",
    "    terminology_version_dropdown.pack(pady=5)\n",
    "    \n",
    "    standard_version_label = tk.Label(tab_2, text=\"Select Standard Version:\", font=(\"Helvetica\", 9), bg=\"white\")\n",
    "    standard_version_label.pack(pady=5)\n",
    "    \n",
    "    global standard_version_dropdown\n",
    "    standard_version_dropdown = ttk.Combobox(tab_2, state=\"readonly\", font=(\"Helvetica\", 11), width=10)\n",
    "    standard_version_dropdown.pack(pady=5)\n",
    "    \n",
    "    def update_versions(event=None):\n",
    "        dataset_type_selected = dataset_type.get()\n",
    "        if dataset_type_selected == \"SDTM\":\n",
    "            terminology_version_dropdown['values'] = [\"SDTM 2025-09-26\", \"SDTM 2025-03-28\", \"SDTM 2024-09-27\"]\n",
    "            standard_version_dropdown['values'] = [\"SDTM IG 3.4\", \"SDTM IG 3.3\", \"SDTM IG 3.2\"]\n",
    "        elif dataset_type_selected == \"ADAM\":\n",
    "            terminology_version_dropdown['values'] = [\"ADAM 2025-09-26\", \"ADAM 2025-03-28\", \"ADAM 2024-09-27\"]\n",
    "            standard_version_dropdown['values'] = [\"ADAM IG 1.3\", \"ADAM IG 1.2\", \"ADAM IG 1.1\"]\n",
    "    \n",
    "        terminology_version_dropdown.set('SDTM 2025-09-26')\n",
    "        standard_version_dropdown.set('SDTM IG 3.4')\n",
    "    \n",
    "    dataset_type_dropdown.bind(\"<<ComboboxSelected>>\", update_versions)\n",
    "\n",
    "    ref_label = tk.Label(tab_2, text=\"Upload Reference Excel file:\", font=app_font, bg=\"white\")\n",
    "    ref_label.pack(pady=10)\n",
    "\n",
    "    ref_button = tk.Button(tab_2, text=\"Upload Reference File\", command=lambda: upload_excel_files('reference'), font=app_font, bg=\"#007BFF\", fg=\"white\", relief=\"raised\")\n",
    "    ref_button.pack(pady=10)\n",
    "\n",
    "    global ref_file_label\n",
    "    ref_file_label = tk.Label(tab_2, text=\"Reference File: Not uploaded\", font=app_font, bg=\"white\", anchor=\"w\", width=40)\n",
    "    ref_file_label.pack(pady=10)\n",
    "\n",
    "    study_label = tk.Label(tab_2, text=\"Upload Study Define Excel file:\", font=app_font, bg=\"white\")\n",
    "    study_label.pack(pady=10)\n",
    "\n",
    "    study_button = tk.Button(tab_2, text=\"Upload Study Define File\", command=lambda: upload_excel_files('study_define'), font=app_font, bg=\"#007BFF\", fg=\"white\", relief=\"raised\")\n",
    "    study_button.pack(pady=10)\n",
    "\n",
    "    global study_file_label\n",
    "    study_file_label = tk.Label(tab_2, text=\"Study Define File: Not uploaded\", font=app_font, bg=\"white\", anchor=\"w\", width=40)\n",
    "    study_file_label.pack(pady=10)\n",
    "\n",
    "    # Delete Dataset Option\n",
    "    dataset_label = tk.Label(tab_2, text=\"Enter Dataset Name to Delete:\", font=app_font, bg=\"white\")\n",
    "    dataset_label.pack(pady=10)\n",
    "\n",
    "    global dataset_delete_entry\n",
    "    dataset_delete_entry = tk.Entry(tab_2, width=40, font=app_font)\n",
    "    dataset_delete_entry.pack(pady=5)\n",
    "\n",
    "    delete_hint = tk.Label(tab_2, text=\"(Optional) Leave blank if you don't want to delete any dataset.\", font=(\"Helvetica\", 8), fg=\"gray\", bg=\"#ADD8E6\")\n",
    "    delete_hint.pack(pady=3)\n",
    "\n",
    "    global start_button\n",
    "    start_button = tk.Button(tab_2, text=\"Start Backend Process\", command=start_backend_process, font=app_font, bg=\"#4CAF50\", fg=\"white\", relief=\"raised\", state=tk.NORMAL)\n",
    "    start_button.pack(pady=10)\n",
    "    \n",
    "    # Tab 3: ISSUES FIXING\n",
    "    tab_3 = ttk.Frame(tab_control)\n",
    "    tab_control.add(tab_3, text=\"Issues Fixing\")\n",
    "    # tab_3.config(bg=\"white\")\n",
    "\n",
    "    issues_excel_label = tk.Label(tab_3, text=\"Upload Issues Excel (CSV):\", font=app_font, bg=\"white\")\n",
    "    issues_excel_label.pack(pady=10)\n",
    "    \n",
    "    issues_excel_button = tk.Button(tab_3, text=\"Upload Issues Excel\", command=lambda: upload_file('issues'), font=app_font, bg=\"#007BFF\", fg=\"white\", relief=\"raised\")\n",
    "    issues_excel_button.pack(pady=10)\n",
    "    \n",
    "    global issues_excel_file_label\n",
    "    issues_excel_file_label = tk.Label(tab_3, text=\"Issues Excel: Not uploaded\", font=app_font, anchor=\"w\", width=40, bg=\"white\")\n",
    "    issues_excel_file_label.pack(pady=10)\n",
    "    \n",
    "    study_define_label = tk.Label(tab_3, text=\"Upload Study Define Excel:\", font=app_font, bg=\"white\")\n",
    "    study_define_label.pack(pady=10)\n",
    "    \n",
    "    study_define_button = tk.Button(tab_3, text=\"Upload Study Define Excel\", command=lambda: upload_file('study_define'), font=app_font, bg=\"#007BFF\", fg=\"white\", relief=\"raised\")\n",
    "    study_define_button.pack(pady=10)\n",
    "    \n",
    "    global study_define_file_label\n",
    "    study_define_file_label = tk.Label(tab_3, text=\"Study Define Excel: Not uploaded\", font=app_font, anchor=\"w\", width=40, bg=\"white\")\n",
    "    study_define_file_label.pack(pady=10)\n",
    "    \n",
    "    start_button = tk.Button(tab_3, text=\"Start Fixing Issues\", command=start_the_process, font=app_font, bg=\"#4CAF50\", fg=\"white\", relief=\"raised\", state=tk.DISABLED)\n",
    "    start_button.pack(pady=10)\n",
    "\n",
    "    # Tab 1: Issues Automation\n",
    "    tab_4 = ttk.Frame(tab_control)\n",
    "    tab_control.add(tab_4, text=\"Issues Automation\")\n",
    "    # tab_1.config(bg=\"white\")  # White background for the tab\n",
    "\n",
    "    study_name_label = tk.Label(tab_4, text=\"Enter Study Name:\", font=app_font, bg=\"white\")\n",
    "    study_name_label.pack(pady=10)\n",
    "\n",
    "    study_name_entry = tk.Entry(tab_4, width=40, font=app_font)\n",
    "    study_name_entry.pack(pady=10)\n",
    "\n",
    "    programmer_name_label = tk.Label(tab_4, text=\"Programmer Name:\", font=app_font, bg=\"white\")\n",
    "    programmer_name_label.pack(pady=10)\n",
    "    \n",
    "    programmer_name_entry = tk.Entry(tab_4, width=40, font=app_font)\n",
    "    programmer_name_entry.pack(pady=10)\n",
    "    \n",
    "    def on_run_selenium():\n",
    "        study_name = study_name_entry.get()\n",
    "        programmer_name = programmer_name_entry.get()\n",
    "        if study_name and programmer_name:\n",
    "            run_selenium_automation(study_name, programmer_name)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Please enter the correct details.\")\n",
    "\n",
    "    run_button = tk.Button(tab_4, text=\"Run Automation\", command=on_run_selenium, font=(\"Helvetica\", 12), bg=\"#4CAF50\", fg=\"white\", relief=\"raised\")\n",
    "    run_button.pack(pady=10)\n",
    "\n",
    "\n",
    "    # Finalizing GUI setup\n",
    "    tab_control.pack(expand=1, fill=\"both\")\n",
    "    root.geometry(\"800x700\")  # Adjust the window size as needed\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_gui()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
